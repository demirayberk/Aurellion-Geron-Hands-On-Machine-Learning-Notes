{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Support Vector Machines\n",
    "\n",
    "- SVM is a powerful and versatile machine learning model, capable of performing linear or nonlinear classification, regression and even **Outlier** detection\n",
    "\n",
    "## Linear SVM classification\n",
    "\n",
    "- SVM is a vector that seperates both classes but for that purpose (if features are linearly seperable) there are infinite amount of lines\n",
    "\n",
    "- Therefore you can think of an SVM classifier as fitting widest possible street between the classes.\n",
    "\n",
    "- It is fully determined (or \"supported\") by the instances located on the edge of the street. These instances are called the support vectors.\n",
    "\n",
    "- Vector is the \"instance\" (from feature) that \"supports\" that's why it is called support vector machine.\n",
    "\n",
    "- SVMs are sensitive to the feature scales.\n",
    "\n",
    "## Soft Margin classification.\n",
    "\n",
    "- If we strictly impose that all instances must be off the street and on the right side this is called hard margin classification.\n",
    "\n",
    "- Missclassification is not allowed in this hard-margin classification.\n",
    "\n",
    "- Hard margin is quite sensitive to outliers and it only works when the data is linearly seperable.\n",
    "\n",
    "- SVM allows for some misclassification or violation of the margin. The objective becomes finding a balance between keeping this misclassification as low as possibel and maximizing the margin. The soft margin approach tends to generalize better to unseen data, particularly when the data is not perfectly linearly seperable.\n",
    "\n",
    "- Flexibility of soft margin classification is controlled by a hyperparameter often denoted s \"C\".\n",
    "\n",
    "- A smaller value of C creates a wider margin but allows more margin violations, a larger value of C creates a narrower margin but allows fewer violations.\n",
    "\n",
    "- SVMs are open to overfitting, so you can try  regularizing it by reducing C.\n",
    "\n",
    "- Unlike logistic regression or decision trees SVM classifiers do not output probabilities for each class.\n",
    "\n",
    "## Nonlinear SVM classification.\n",
    "\n",
    "- One approach handling nonlinear datasets is to add more features such as polynomial features (like in linear regression).\n",
    "\n",
    "- You can get the same result by setting SVC kernel to poly and setting degree\n",
    "\n",
    "- The hyperparameter coef0 controls how much the model is influenced by high degree polynomials versus low degree polynomials\n",
    "\n",
    "- A common approach to finding the right hyperparameter values is to use grid search. It is often faster to first do a very coarse grid search then a finer grid search around the beset values found. Having a good sense fo what each hyperparameter actually does can also help you to search in the right part of the hyperparameter space.\n",
    "\n",
    "- Just like the polynomial features method the similarity features can be useful with any machine learning algorithm, but it may be computationally expensive to compute all the additional features. especially on large training sets.\n",
    "\n",
    "- Gussian RBF KERNEL (similarity function) has gamma and C hyperparameters\n",
    "\n",
    "- Increasing gamma makes the bell-shaped curve narrower. As a result each instance's range of influence is smaller: the decision boundary ends up being more irregular. Conversely a small gamma value makes the bell shaped cure wider. If your model is overfitting you should reduce it; if it is underfitting you should increase it (similiar to X hyperparameter).\n",
    "\n",
    "- As a rule of thumb you should always try the linear kernel first. especially if the training set is very large or if it has plenty of features. If the training set is not too large you shouild also try the Gaussoan RBF kernel; it works well in most cases. Then if you have spare time and computing power, you can experimentwith ferw other kernels using cross validation and grid search you;d want to experiment like that especially if there are kernels specialized for your training set;s data structure.\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

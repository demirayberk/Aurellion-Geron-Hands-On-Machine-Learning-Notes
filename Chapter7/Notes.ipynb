{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CHAPTER7: ENSEMBLE LEARNING AND RANDOM FORESTS.\n",
    "\n",
    "## Notes\n",
    "\n",
    "- A group of predictors is called an ensemble; this, this technique is called ***Ensemble Learning***\n",
    "\n",
    "- As an example of an Ensemble method you can train a group of decision tree classifiers each on a different random subset of the training set. To make predictions you obtain the predictions of all the individuyal trees then predict the class that gets the most votes. Such an ensemble of decision trees is called a random forest and despite its simplicity this is one of the most powerful mlk algorithms available today.\n",
    "\n",
    "### Voting Classifiers\n",
    "\n",
    "- This method is somewhat a straight forward one, you train the same subset of the training data with the different Predictors and than you aggregate their results\n",
    "\n",
    "- If its a classifing problem you take the mode of the predictions, if it is a regression problem you take the mean of the different regressors in ensemble learning.\n",
    "\n",
    "- Ensemble methods work best when the predictors are as independent from one another as possible. One way to get diverse classifiers is to train them using very different algorithms. This increases the chance that they will make very different types of errors, improving the ensemble's accuracy.\n",
    "\n",
    "#### Example Voting classifier\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.datasets import make_moons\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the training set.\n",
    "\n",
    "X, y = make_moons(n_samples=500, noise=0.30, random_state=42)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>VotingClassifier(estimators=[(&#x27;lr&#x27;, LogisticRegression(penalty=None)),\n",
       "                             (&#x27;rand_forest&#x27;, RandomForestClassifier()),\n",
       "                             (&#x27;svm&#x27;, SVC())])</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" ><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">VotingClassifier</label><div class=\"sk-toggleable__content\"><pre>VotingClassifier(estimators=[(&#x27;lr&#x27;, LogisticRegression(penalty=None)),\n",
       "                             (&#x27;rand_forest&#x27;, RandomForestClassifier()),\n",
       "                             (&#x27;svm&#x27;, SVC())])</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><label>lr</label></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" ><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LogisticRegression</label><div class=\"sk-toggleable__content\"><pre>LogisticRegression(penalty=None)</pre></div></div></div></div></div></div><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><label>rand_forest</label></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" ><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">RandomForestClassifier</label><div class=\"sk-toggleable__content\"><pre>RandomForestClassifier()</pre></div></div></div></div></div></div><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><label>svm</label></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-4\" type=\"checkbox\" ><label for=\"sk-estimator-id-4\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">SVC</label><div class=\"sk-toggleable__content\"><pre>SVC()</pre></div></div></div></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "VotingClassifier(estimators=[('lr', LogisticRegression(penalty=None)),\n",
       "                             ('rand_forest', RandomForestClassifier()),\n",
       "                             ('svm', SVC())])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_clf = LogisticRegression(penalty=None)\n",
    "rnd_clf = RandomForestClassifier()\n",
    "svm_clf = SVC()\n",
    "\n",
    "voting_clf = VotingClassifier(estimators=[(\"lr\", log_clf),\n",
    "                                          (\"rand_forest\", rnd_clf),\n",
    "                                          (\"svm\", svm_clf)],\n",
    "                              voting=\"hard\")\n",
    "\n",
    "voting_clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.904"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Voting classifier outperforms all of the classifiers.\n",
    "voting_clf.score(X_test, y_test)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- If all classifiers are able to estimate class probabilities (they have a preict_proba method) then you can tell scikit learn to predict the class with the highest class probablity averaged over all the individual classifiers. This is called soft voting it often achieves higher performance than hard voting because it gives more weight to highly confident votes. All you need to do is replace voting=\"hard\"  with the voting=\"soft\" and ensure that all classifiers can estimate class probabilities.\n",
    "\n",
    "### Bagging and Pasting.\n",
    "\n",
    "- Bagging and pasting approach is to use the same training algorithm for every predictor and train them on different random subsets of the training set. When sampling is performed with replacement this method is called bagging. When sampling is performed without replacement it is called pasting.\n",
    "\n",
    "- Once all predictors are trained, the enseble can make a prediction for a new instance by simply aggregating the predictions of all predictors. The aggregation function is typically the statistical mode, for classification. or the average for regression. Generallt the net result is that the ensemble has a similar bias but a lower variance than a single predictor trained on the original training set.\n",
    "\n",
    "#### Example bagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.904"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "bag_clf = BaggingClassifier(DecisionTreeClassifier(), n_estimators=100,\n",
    "                            bootstrap=True, random_state=1337)\n",
    "bag_clf.fit(X_train, y_train)\n",
    "bag_clf.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.1s remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END .................................................... total time=   0.1s\n",
      "[CV] END .................................................... total time=   0.1s\n",
      "[CV] END .................................................... total time=   0.1s\n",
      "[CV] END .................................................... total time=   0.0s\n",
      "[CV] END .................................................... total time=   0.1s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   5 out of   5 | elapsed:    0.7s finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0.86666667, 0.92      , 0.88      , 0.88      , 0.88      ])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "## using bagging model\n",
    "\n",
    "scores = cross_val_score(bag_clf, X_train, y_train, cv=5, verbose=2)\n",
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.84      , 0.89333333, 0.86666667, 0.85333333, 0.86666667])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "past_clf = BaggingClassifier(DecisionTreeClassifier(), n_estimators=100,\n",
    "                            max_samples=1.0, bootstrap=False, random_state=1337)\n",
    "\n",
    "scores = cross_val_score(past_clf, X_train, y_train,\n",
    "                         cv=5)\n",
    "scores"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- On average about 63 % is sampled in the bagging method for each independent predictor. But dont forget that size of the sample is generally the same with the dataset that means there are duplicates of rows in the sample for each predictor\n",
    "\n",
    "- Also that means that 37 % of the instances from dataset a predictor does not see that means, we can evaluate our predictor with that 37% you can set oob(out of bag)_on = True and you can see it through oob_score_ attribute.\n",
    "\n",
    "#### Example OOB score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.896, 0.904, 1.0)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bag_clf = BaggingClassifier(DecisionTreeClassifier(), \n",
    "                            n_estimators=100,\n",
    "                            bootstrap=True, \n",
    "                            oob_score=True,\n",
    "                            random_state=1337)\n",
    "\n",
    "\n",
    "bag_clf.fit(X_train, y_train)\n",
    "bag_clf.oob_score_, bag_clf.score(X_test, y_test), bag_clf.score(X_train, y_train)\n",
    "# This results in average of the oob evalutations of each predictor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.38095238, 0.61904762],\n",
       "       [0.45      , 0.55      ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.05128205, 0.94871795],\n",
       "       [0.31428571, 0.68571429],\n",
       "       [0.        , 1.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.89473684, 0.10526316],\n",
       "       [0.79487179, 0.20512821],\n",
       "       [0.        , 1.        ],\n",
       "       [0.80952381, 0.19047619],\n",
       "       [0.91176471, 0.08823529],\n",
       "       [0.97560976, 0.02439024],\n",
       "       [0.02857143, 0.97142857],\n",
       "       [0.02777778, 0.97222222],\n",
       "       [1.        , 0.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.02941176, 0.97058824],\n",
       "       [0.28571429, 0.71428571],\n",
       "       [0.975     , 0.025     ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.925     , 0.075     ],\n",
       "       [0.        , 1.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.62857143, 0.37142857],\n",
       "       [0.        , 1.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.07894737, 0.92105263],\n",
       "       [1.        , 0.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.23529412, 0.76470588],\n",
       "       [0.        , 1.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.21875   , 0.78125   ],\n",
       "       [0.18181818, 0.81818182],\n",
       "       [1.        , 0.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.025     , 0.975     ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.97222222, 0.02777778],\n",
       "       [0.91666667, 0.08333333],\n",
       "       [0.94736842, 0.05263158],\n",
       "       [1.        , 0.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.0625    , 0.9375    ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.76315789, 0.23684211],\n",
       "       [0.425     , 0.575     ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.71794872, 0.28205128],\n",
       "       [1.        , 0.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.80645161, 0.19354839],\n",
       "       [1.        , 0.        ],\n",
       "       [0.63333333, 0.36666667],\n",
       "       [0.16326531, 0.83673469],\n",
       "       [0.66666667, 0.33333333],\n",
       "       [0.9375    , 0.0625    ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.19512195, 0.80487805],\n",
       "       [0.84848485, 0.15151515],\n",
       "       [1.        , 0.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.05263158, 0.94736842],\n",
       "       [0.        , 1.        ],\n",
       "       [0.36585366, 0.63414634],\n",
       "       [1.        , 0.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.70588235, 0.29411765],\n",
       "       [0.        , 1.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.29411765, 0.70588235],\n",
       "       [1.        , 0.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.9047619 , 0.0952381 ],\n",
       "       [0.9047619 , 0.0952381 ],\n",
       "       [0.        , 1.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.18604651, 0.81395349],\n",
       "       [0.61764706, 0.38235294],\n",
       "       [0.        , 1.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.61538462, 0.38461538],\n",
       "       [1.        , 0.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.23333333, 0.76666667],\n",
       "       [0.43243243, 0.56756757],\n",
       "       [1.        , 0.        ],\n",
       "       [0.02631579, 0.97368421],\n",
       "       [1.        , 0.        ],\n",
       "       [0.23076923, 0.76923077],\n",
       "       [0.94444444, 0.05555556],\n",
       "       [1.        , 0.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.78571429, 0.21428571],\n",
       "       [1.        , 0.        ],\n",
       "       [0.03333333, 0.96666667],\n",
       "       [1.        , 0.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.93023256, 0.06976744],\n",
       "       [1.        , 0.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.2       , 0.8       ],\n",
       "       [0.97058824, 0.02941176],\n",
       "       [0.34210526, 0.65789474],\n",
       "       [1.        , 0.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.65789474, 0.34210526],\n",
       "       [0.41463415, 0.58536585],\n",
       "       [0.36363636, 0.63636364],\n",
       "       [0.91176471, 0.08823529],\n",
       "       [0.97222222, 0.02777778],\n",
       "       [0.05128205, 0.94871795],\n",
       "       [0.79487179, 0.20512821],\n",
       "       [0.        , 1.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.97435897, 0.02564103],\n",
       "       [1.        , 0.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.93023256, 0.06976744],\n",
       "       [1.        , 0.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.38461538, 0.61538462],\n",
       "       [0.21875   , 0.78125   ],\n",
       "       [0.02857143, 0.97142857],\n",
       "       [0.        , 1.        ],\n",
       "       [0.4       , 0.6       ],\n",
       "       [1.        , 0.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.64705882, 0.35294118],\n",
       "       [0.85294118, 0.14705882],\n",
       "       [0.        , 1.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.08571429, 0.91428571],\n",
       "       [1.        , 0.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.87096774, 0.12903226],\n",
       "       [0.75757576, 0.24242424],\n",
       "       [0.72727273, 0.27272727],\n",
       "       [0.        , 1.        ],\n",
       "       [0.10810811, 0.89189189],\n",
       "       [1.        , 0.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.96774194, 0.03225806],\n",
       "       [1.        , 0.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.61111111, 0.38888889],\n",
       "       [0.71875   , 0.28125   ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.025     , 0.975     ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.97222222, 0.02777778],\n",
       "       [0.        , 1.        ],\n",
       "       [0.36585366, 0.63414634],\n",
       "       [0.        , 1.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.97619048, 0.02380952],\n",
       "       [0.7826087 , 0.2173913 ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.05      , 0.95      ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.02777778, 0.97222222],\n",
       "       [0.        , 1.        ],\n",
       "       [0.10526316, 0.89473684],\n",
       "       [1.        , 0.        ],\n",
       "       [0.80952381, 0.19047619],\n",
       "       [0.        , 1.        ],\n",
       "       [0.90243902, 0.09756098],\n",
       "       [0.97435897, 0.02564103],\n",
       "       [0.16666667, 0.83333333],\n",
       "       [0.16666667, 0.83333333],\n",
       "       [1.        , 0.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.31034483, 0.68965517],\n",
       "       [0.97058824, 0.02941176],\n",
       "       [0.        , 1.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.58974359, 0.41025641],\n",
       "       [1.        , 0.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.06976744, 0.93023256],\n",
       "       [0.12121212, 0.87878788],\n",
       "       [0.94285714, 0.05714286],\n",
       "       [0.        , 1.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.34210526, 0.65789474],\n",
       "       [0.11363636, 0.88636364],\n",
       "       [0.48780488, 0.51219512],\n",
       "       [0.5952381 , 0.4047619 ],\n",
       "       [0.        , 1.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.63636364, 0.36363636],\n",
       "       [0.        , 1.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.30434783, 0.69565217],\n",
       "       [0.86206897, 0.13793103],\n",
       "       [0.03448276, 0.96551724],\n",
       "       [1.        , 0.        ],\n",
       "       [0.825     , 0.175     ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.05263158, 0.94736842],\n",
       "       [0.        , 1.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.96428571, 0.03571429],\n",
       "       [0.29032258, 0.70967742],\n",
       "       [0.83870968, 0.16129032],\n",
       "       [0.        , 1.        ],\n",
       "       [0.5       , 0.5       ],\n",
       "       [0.10810811, 0.89189189],\n",
       "       [1.        , 0.        ],\n",
       "       [0.82926829, 0.17073171],\n",
       "       [0.        , 1.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.94444444, 0.05555556],\n",
       "       [0.        , 1.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.40540541, 0.59459459],\n",
       "       [1.        , 0.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.8125    , 0.1875    ],\n",
       "       [0.        , 1.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.725     , 0.275     ],\n",
       "       [0.94285714, 0.05714286],\n",
       "       [1.        , 0.        ],\n",
       "       [0.75      , 0.25      ],\n",
       "       [0.48648649, 0.51351351],\n",
       "       [0.        , 1.        ],\n",
       "       [0.88571429, 0.11428571],\n",
       "       [0.        , 1.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.89189189, 0.10810811],\n",
       "       [1.        , 0.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.84848485, 0.15151515],\n",
       "       [0.17142857, 0.82857143],\n",
       "       [0.56097561, 0.43902439],\n",
       "       [0.17142857, 0.82857143],\n",
       "       [0.        , 1.        ],\n",
       "       [0.94594595, 0.05405405],\n",
       "       [0.93548387, 0.06451613],\n",
       "       [0.        , 1.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.03448276, 0.96551724],\n",
       "       [0.95121951, 0.04878049],\n",
       "       [1.        , 0.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.47619048, 0.52380952],\n",
       "       [1.        , 0.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.93333333, 0.06666667],\n",
       "       [0.05555556, 0.94444444],\n",
       "       [1.        , 0.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.87179487, 0.12820513],\n",
       "       [0.        , 1.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.05555556, 0.94444444],\n",
       "       [1.        , 0.        ],\n",
       "       [0.07692308, 0.92307692],\n",
       "       [0.        , 1.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.375     , 0.625     ],\n",
       "       [0.125     , 0.875     ],\n",
       "       [0.39393939, 0.60606061],\n",
       "       [1.        , 0.        ],\n",
       "       [0.975     , 0.025     ],\n",
       "       [0.27272727, 0.72727273],\n",
       "       [1.        , 0.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.9047619 , 0.0952381 ],\n",
       "       [0.43589744, 0.56410256],\n",
       "       [0.96969697, 0.03030303],\n",
       "       [1.        , 0.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.66666667, 0.33333333]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bag_clf.oob_decision_function_"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The oob decision function for each training instance is also available through the oob_decusuib_function variable. In this case the decision functions returns the class probabilities for each training instance.\n",
    "\n",
    "- Itreturns the avarege oob scores for each training instance."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Max samples and bootstrap are connected just like it, max_features and bootstrap features are connected\n",
    "\n",
    "- Max_features controls how many features are going to be selected, and bootstrap_features controls if replacement after sampling the features are going to happen. It is set the false default so there will not be duplicates of the features.\n",
    "\n",
    "- Sampling both training instances and features is called the Random patches method. keeping all training instances but sampling features is called random subspaces.\n",
    "\n",
    "#### Random Forests\n",
    "\n",
    "- Instead of building a baggingclassifier and poassing it a decisiontreeclassifier you can istead use the randomforestclassifier class which is more convenient and optimized for decision trees.\n",
    "\n",
    "- The Random Forest algorithm introduces extra randomness when growing trees instead of searching the very best feature when splitting a node it searches for the best feature among a random subset of features. The algorithm results in greater tree diversity, which trades a higher bias for a lower variance, generally yielding an overall better model.\n",
    "\n",
    "- Another great quality of the random forests is they make it easy to measure the relative importance of each feature.\n",
    "\n",
    "- YOu can axxes the result using the feature_importances_ variable.\n",
    "\n",
    "- Random Forests are very handy to get a quick understanding of what features actually matter in particular if you need to perform feature selection.\n",
    "\n",
    "#### Boosting\n",
    "\n",
    "- The general idea of most boosting methods is to train predictors sequentially, each trying to correct ist predeccor.\n",
    "\n",
    "- One of the most popular is adaboosting, aka gradient boosting.\n",
    "\n",
    "- One way for a new predictor to correct its predecessor is to pay a bit more attention to the training instances that the predecessor underfitted this results in new predictors focusing more and more on the hard cases. This is the technique used by AdaBoost.\n",
    "\n",
    "- Once all predictors are trained, the ensemble makes predictions very much like bagging or pasting, except that predictors have different weights depending on their overall accuracy on the weighted training set.\n",
    "\n",
    "- In gradient boosting method tries to fit the new predictor to the residual errors made by the previous predictor.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "\n",
    "ada_clf = AdaBoostClassifier(\n",
    "    DecisionTreeClassifier(max_depth=1), n_estimators=200,\n",
    "    learning_rate=0.5\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
